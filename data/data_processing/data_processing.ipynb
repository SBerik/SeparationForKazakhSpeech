{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb38771-56e7-450c-a733-03c226d694cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import re \n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from functional import *\n",
    "from overlap import get_trimmed_sample, overlap_using_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b046d50-620f-4bfb-ba3a-481f61859f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e7b9dc-1619-419c-aa1d-ea8213ec82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\"sample1\": \"C:/Users/b.smadiarov/Diploma/DualPathRNN/samples/5ed8a1c0f3ea2.flac\",\n",
    "         \"sample2\": \"C:/Users/b.smadiarov/Diploma/DualPathRNN/samples/5f2b0a06ceb18.flac\",\n",
    "         \n",
    "         'KS2': 'F:/ISSAI_KSC2_unpacked/ISSAI_KSC2/',\n",
    "         'root': 'F:/ISSAI_KSC2_unpacked',\n",
    "         \n",
    "         'src_csv': 'C:/Users/b.smadiarov/Diploma/VoiceDetectionForKazakhSpeech/data/annotation/Csv', \n",
    "\n",
    "         'trimmed': 'F:/ISSAI_KSC2_unpacked/trimmed', \n",
    "         \"mixed\": \"C:/Users/b.smadiarov/Diploma/DualPathRNN/data/mixed\", \n",
    "         \"samples\": \"C:/Users/b.smadiarov/Diploma/DualPathRNN/samples\",\n",
    "         \"tts\": 'F:/ISSAI_KSC2_unpacked/trimmed/Train/tts',\n",
    "\n",
    "         'dihard_data': 'F:/ISSAI_KSC2_unpacked/diahard_data' \n",
    "        }\n",
    "\n",
    "glob_paths = {'KS2_raw': 'F:/ISSAI_KSC2_unpacked/ISSAI_KSC2/**/**/*.flac',\n",
    "              \"mixed_csv\": \"C:/Users/b.smadiarov/Diploma/DualPathRNN/data/mixed/*.csv\",\n",
    "              \"mixed_flac\": \"C:/Users/b.smadiarov/Diploma/DualPathRNN/data/mixed/*.flac\",\n",
    "\n",
    "              \"f_real_flac\": 'F:/ISSAI_KSC2_unpacked/trimmed/**/**/*.flac',\n",
    "              \"tts\": 'F:/ISSAI_KSC2_unpacked/trimmed/Train/tts/*.flac', \n",
    "             } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd2aaf-2116-4054-88f9-b56ad3e71266",
   "metadata": {},
   "source": [
    "#### Generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c29d2a-d651-4f99-a879-4e21fd6c5d91",
   "metadata": {},
   "source": [
    "ToDO: \n",
    "- Короткие записи сливались с короткими записями.\n",
    "\n",
    "Критерий: нету посторонних звуков \n",
    "1. tts - только для text_to_speech\n",
    "2. Самый лучший crowdsourced,\n",
    "3. tv_news: эпортаж это обычно речь \n",
    "4. parlament: т.к. кричат с требун  \n",
    "5. Ужасные - radio: т.к. играет вечно музыка, talkshow - тоже какая то дичь, podcast - казахские подкасты это не подкасты а радио."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec1ae2-a94d-46ba-80a6-4844cac888e0",
   "metadata": {},
   "source": [
    "Кроме памяти (мне ее жалко) и так же в аннотационных файлах указана путь до файла. Поэтому что бы придестеречься \n",
    "я решил оставить все как есть."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fc464-21f5-4b4b-88d7-e66c1d72c5cc",
   "metadata": {},
   "source": [
    "##### **1. Убираем начало и конец.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730c1aa-85c8-47fa-9f93-f83f0c9a62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracklist = get_audios_with_folder_name(['crowdsourced', 'tts', 'tv_news'], glob_paths['KS2_raw'])\n",
    "print('Dataset total len:', len(tracklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fb8e59-230b-42ff-ad81-a9dcc74a27ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2c55d5700f492b932aeaab97b26752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/516458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t in tqdm(tracklist):\n",
    "    trimmed_audio = get_trimmed_sample(t, sr, paths['src_csv'])\n",
    "    folder = t.replace(\"ISSAI_KSC2/\", \"trimmed/\").replace(f\"/{get_file_name (t)}.flac\", '')\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    path_to_save = folder + f\"/{get_file_name (t)}.flac\"\n",
    "    sf.write(path_to_save, trimmed_audio, samplerate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a522e8-b6bd-41d5-b923-9311b092c03d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **2. Объединяем аудиодорожки**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d5d7b-cbf5-4cc2-b6ed-f707de4bdfdd",
   "metadata": {},
   "source": [
    "Pipline: \n",
    "1. Есть большой набор данных - trimmed в нем только самые лучшие для задачи диаризации аудодорожки. Сэмплов достаточно много - поэтому будет переменная `total_amount`, которая будет брать уже от trimmed папки.\n",
    "2. Далее т.к. мы объединяем аудиодорожки то нам нужно их по отдельности обрезанное сохранить или запомнить их общую длину в csv файлы. В csv файле будет храниться столбец `min_length`который будет означать общую минимальную длину. Это поможет экономить место на диске. Но из за этого будет долше проходить инициализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41742431-8c6e-4eaf-80bd-9c850151f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_amount_percent = 0.1 # 10 %\n",
    "# Примерное число которое я хочу ~ 2000 записей для начало\n",
    "total_amount = 2000\n",
    "k = 2\n",
    "seed = k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee4c5b-f9bb-4453-9226-135c2a5ae608",
   "metadata": {},
   "source": [
    "1. Число класстеров _k_\n",
    "2. _Seed_ зависит от числа кластеров. Это сделанно для того что бы добавить не предусмотренность. Но таким образом есть опасность что модель просто выучит голоса нежели начнет.\n",
    "3. Для каждого числа класстера можно брать разный датасет.\n",
    "4. Перемешивать нужно для того \"говорящие\" не шли одним за другим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd473e47-93ea-40bf-a846-bb9571c4876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracklist = glob(glob_paths['tts'])\n",
    "# tracklist = tracklist [:int(len(tracklist)*total_amount)]\n",
    "tracklist = tracklist [:total_amount] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab67492-1392-46ab-b53d-c3423ca1f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.shuffle(tracklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b18d6dc-f9e2-4e72-8897-f48f89b1d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{paths['root']}/mixed_{k}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df426f-458e-4bdb-8306-fea06067aae8",
   "metadata": {},
   "source": [
    "Смешивание файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a070fec6-db10-4d37-946c-17f602344549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2c0dc374bb4287b20a32f8268be880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Этот метод не нравится т.к. хочется что бы дорожка была как можно длинее.\n",
    "# То есть аудио с одинаковой длины объединялись с одинаковыми аудио.\n",
    "\n",
    "for i in tqdm(range(0, len(tracklist), k)):\n",
    "    tracks = tracklist[i:i+k]\n",
    "    tracks = sorted(tracks)\n",
    "    overlay, min_length = overlap_using_min(tracks)\n",
    "    audio_names = \"_\".join([get_file_name(t) for t in tracks])\n",
    "    audio_name = f\"{paths['root']}/mixed_{k}/{audio_names}.flac\"\n",
    "    sf.write(f\"{audio_name}\", overlay, samplerate=16000)\n",
    "    df = pd.DataFrame({'mixed_audio': [audio_name],\n",
    "                       'common_len_idx': [min_length],\n",
    "                       **{f'audio_{j+1}': [t.replace('\\\\', '/')] for j, t in enumerate(tracks)}})\n",
    "    df.to_csv(f\"{audio_name.replace('.flac', '.csv')}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8f87a3-c45d-438c-b73f-a03c4b0aebfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396eee01d72e43efa954815c830984bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get full dataset DF\n",
    "annotations = glob(f\"F:/ISSAI_KSC2_unpacked/mixed_{k}/*.csv\")\n",
    "all_dfs = []\n",
    "for csv in tqdm(annotations):\n",
    "    df = pd.read_csv(csv)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "concated_df = pd.concat(all_dfs).reset_index(drop=True)\n",
    "os.makedirs(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO\", exist_ok=True)\n",
    "concated_df.to_csv(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/CONCATED_DFS_tts=2000_k_{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da758c9-998e-4a71-b309-f0f133e397a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed num files: 1000 for k = 2 clusters, total_amount_files: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mixed num files: {len(concated_df)} for k = {k} clusters, total_amount_files: {len(tracklist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a57d1",
   "metadata": {},
   "source": [
    "#### To get need to data-format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8af9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# папка где хранять исходники для дириазации - они обрезанные без начало 'F:/ISSAI_KSC2_unpacked/trimmed',\n",
    "# mixed_2 где хранять смешанные аудиодорожки для двух класстеров\n",
    "# осторожно - нету папки где храняться targetы - вот эти и займемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Создаем все targetы\n",
    "# 2. generate `.scp` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef0f6-7265-43e5-a5f2-e924800a4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Есть csv файл нужно \n",
    "# Я хочу сохранить audio_1, audio_2 в отдельные папки \n",
    "# Папки будет две - одна tr - train, другая cv - validation\n",
    "# В каждой внутри папки есть mix и "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b131587d-75ad-4b2d-aa53-78f249e6271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Делим один csv файла на два. \n",
    "k = 2\n",
    "csv_path = f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/CONCATED_DFS_tts=2000_k_{2}.csv\"\n",
    "all_df = pd.read_csv(csv_path)\n",
    "half_index = int(len(all_df) * 0.8)\n",
    "df1 = all_df.iloc[:half_index]\n",
    "df2 = all_df.iloc[half_index:]\n",
    "df1.to_csv(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/train_k_{k}.csv\", index=False)\n",
    "df2.to_csv(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/valid_k_{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7094e2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e395c83dbae24d80a5f4342c832ee5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 2\n",
    "csv_path = f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/train_k_{k}.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    mixed_audio_path = row.iloc[0]\n",
    "    stage = get_file_name(csv_path).split('_')[0]\n",
    "    assert stage in ['train', 'valid'] \n",
    "    new_mx_audio_path = paths['dihard_data'] + f\"/mixed_{k}/{stage}/mix/\" + get_file_name (mixed_audio_path) + '.flac'\n",
    "    shutil.move(mixed_audio_path, new_mx_audio_path)\n",
    "    common_len = row.iloc[1]\n",
    "    for column in range(2, len(full_df.columns)):\n",
    "        raw_pth_name = row.iloc[column]\n",
    "        audio, sr = librosa.load(raw_pth_name, sr=sr)\n",
    "        stage = get_file_name(csv_path).split('_')[0]\n",
    "        assert stage in ['train', 'valid'] \n",
    "        folder_name = f\"{paths['dihard_data']}/mixed_{k}/{stage}/s{column-k+1}\"\n",
    "        new_path_file_name = f\"{folder_name}/{get_file_name(raw_pth_name)}.flac\"\n",
    "        sf.write(f\"{new_path_file_name}\", audio[:common_len], samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d74c7-12a4-4a71-bc8d-e9939722a81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "csv_path = f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/valid_k_{k}.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    mixed_audio_path = row.iloc[0]\n",
    "    stage = get_file_name(csv_path).split('_')[0]\n",
    "    assert stage in ['train', 'valid'] \n",
    "    new_mx_audio_path = paths['dihard_data'] + f\"/mixed_{k}/{stage}/mix/\" + get_file_name (mixed_audio_path) + '.flac'\n",
    "    shutil.move(mixed_audio_path, new_mx_audio_path)\n",
    "    common_len = row.iloc[1]\n",
    "    for column in range(2, len(full_df.columns)):\n",
    "        raw_pth_name = row.iloc[column]\n",
    "        audio, sr = librosa.load(raw_pth_name, sr=sr)\n",
    "        stage = get_file_name(csv_path).split('_')[0]\n",
    "        assert stage in ['train', 'valid'] \n",
    "        folder_name = f\"{paths['dihard_data']}/mixed_{k}/{stage}/s{column-k+1}\"\n",
    "        new_path_file_name = f\"{folder_name}/{get_file_name(raw_pth_name)}.flac\"\n",
    "        sf.write(f\"{new_path_file_name}\", audio[:common_len], samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a53fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "import os\n",
    "\n",
    "# Пути и файлы для записи\n",
    "datasets = {\n",
    "    \"train\": {\n",
    "        \"mix\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/train/mix\",\n",
    "        \"s1\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/train/s1\",\n",
    "        \"s2\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/train/s2\",\n",
    "        \"scp\": {\n",
    "            \"mix\": \"tr_mix.scp\",\n",
    "            \"s1\": \"tr_s1.scp\",\n",
    "            \"s2\": \"tr_s2.scp\",\n",
    "        },\n",
    "    },\n",
    "    \"cv\": {\n",
    "        \"mix\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/valid/mix\",\n",
    "        \"s1\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/valid/s1\",\n",
    "        \"s2\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/valid/s2\",\n",
    "        \"scp\": {\n",
    "            \"mix\": \"cv_mix.scp\",\n",
    "            \"s1\": \"cv_s1.scp\",\n",
    "            \"s2\": \"cv_s2.scp\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Функция для записи данных в SCP-файл\n",
    "def write_scp(input_dir, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            files.sort()\n",
    "            for file in files:\n",
    "                f.write(f\"{file} {os.path.join(root, file)}\\n\")\n",
    "\n",
    "# Генерация SCP-файлов для всех наборов данных\n",
    "for split, paths in datasets.items():\n",
    "    for key, input_dir in paths.items():\n",
    "        if key == \"scp\":  # Пропустить словарь с именами файлов\n",
    "            continue\n",
    "        write_scp(input_dir, paths[\"scp\"][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4883c7-37b0-4671-bdcf-e4ef330f2be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
