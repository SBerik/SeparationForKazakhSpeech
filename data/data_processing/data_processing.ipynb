{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb38771-56e7-450c-a733-03c226d694cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import re \n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from functional import *\n",
    "from overlap import get_trimmed_sample, overlap_using_min\n",
    "from predictor import get_VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b046d50-620f-4bfb-ba3a-481f61859f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e7b9dc-1619-419c-aa1d-ea8213ec82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {'root': 'F:/ISSAI_KSC2_unpacked',\n",
    "         'src_csv': 'C:/Users/b.smadiarov/Diploma/VoiceDetectionForKazakhSpeech/data/annotation/Csv', \n",
    "         'trimmed': 'F:/ISSAI_KSC2_unpacked/trimmed', \n",
    "         'dihard_data': 'F:/ISSAI_KSC2_unpacked/diahard_data' \n",
    "        }\n",
    "\n",
    "glob_paths = {'KS2_raw': 'F:/ISSAI_KSC2_unpacked/ISSAI_KSC2/**/**/*.flac',\n",
    "              \"trimmed\": 'F:/ISSAI_KSC2_unpacked/trimmed/**/**/*.flac'\n",
    "             } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd2aaf-2116-4054-88f9-b56ad3e71266",
   "metadata": {},
   "source": [
    "#### Generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c29d2a-d651-4f99-a879-4e21fd6c5d91",
   "metadata": {},
   "source": [
    "ToDO: \n",
    "- Короткие записи сливались с короткими записями.\n",
    "\n",
    "Критерий: нету посторонних звуков \n",
    "1. tts - только для text_to_speech\n",
    "2. Самый лучший crowdsourced,\n",
    "3. tv_news: эпортаж это обычно речь \n",
    "4. parlament: т.к. кричат с требун  \n",
    "5. Ужасные - radio: т.к. играет вечно музыка, talkshow - тоже какая то дичь, podcast - казахские подкасты это не подкасты а радио."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec1ae2-a94d-46ba-80a6-4844cac888e0",
   "metadata": {},
   "source": [
    "Кроме памяти (мне ее жалко) и так же в аннотационных файлах указана путь до файла. Поэтому что бы придестеречься \n",
    "я решил оставить все как есть."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fc464-21f5-4b4b-88d7-e66c1d72c5cc",
   "metadata": {},
   "source": [
    "#### **1. Trimmed files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cefaaad-0a98-4724-9ee7-ad2b56cc6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset total len: 516458\n"
     ]
    }
   ],
   "source": [
    "tracklist = get_audios_with_folder_name(['crowdsourced', 'tts', 'tv_news'], glob_paths['KS2_raw'])\n",
    "print('Dataset total len:', len(tracklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0804f1fa-7cf5-448f-977c-b59f3a67eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 516458/516458 [03:18<00:00, 2607.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with need structer: 145138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matching_files = []\n",
    "\n",
    "for track in tqdm(tracklist):\n",
    "    track = track.replace('\\\\', '/')\n",
    "    csv_file = get_VAD(track, paths['src_csv'], sr=sr)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if len(df) == 3:  \n",
    "        speech_sequence = df['speech'].values[:3]\n",
    "        if list(speech_sequence) == [0, 1, 0]:\n",
    "            matching_files.append(track)\n",
    "    \n",
    "print(\"File with need structer:\", len(matching_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115917f2-1716-4fd2-95e4-19afdadc83ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:/ISSAI_KSC2_unpacked/ISSAI_KSC2/Dev/crowdsourced/5f55bbc1759d2.flac',\n",
       " 'F:/ISSAI_KSC2_unpacked/ISSAI_KSC2/Dev/crowdsourced/5f55bbc87ca87.flac']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_files[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f397e30-fc27-4a47-9280-65298bd3e153",
   "metadata": {},
   "source": [
    "Saving trimmed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b149e415-bf34-434e-8f49-b7e496da575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 145138/145138 [18:56<00:00, 127.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(matching_files):\n",
    "    trimmed_audio = get_trimmed_sample(t, sr, paths['src_csv'])\n",
    "    folder = t.replace(\"ISSAI_KSC2/\", \"trimmed/\").replace(f\"/{get_file_name (t)}.flac\", '')\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    path_to_save = folder + f\"/{get_file_name (t)}.flac\"\n",
    "    sf.write(path_to_save, trimmed_audio, samplerate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d18de6-4c73-418a-a93e-ea882f2240ab",
   "metadata": {},
   "source": [
    "Single Class Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9591c840-ef34-48da-a722-727b95aad982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 516458/516458 [09:36<00:00, 896.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(tracklist):\n",
    "    t = t.replace('\\\\', '/')\n",
    "    csv_file = get_VAD(t, paths['src_csv'], sr=sr)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if isSingleClassSample(df):\n",
    "        audio, sr = librosa.load(t, sr=sr)\n",
    "        folder = t.replace(\"ISSAI_KSC2/\", \"trimmed/\").replace(f\"/{get_file_name (t)}.flac\", '')\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        path_to_save = folder + f\"/{get_file_name (t)}.flac\"\n",
    "        sf.write(path_to_save, trimmed_audio, samplerate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a522e8-b6bd-41d5-b923-9311b092c03d",
   "metadata": {},
   "source": [
    "#### **2. Объединяем аудиодорожки**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d5d7b-cbf5-4cc2-b6ed-f707de4bdfdd",
   "metadata": {},
   "source": [
    "Pipline: \n",
    "1. Есть большой набор данных - trimmed в нем только самые лучшие для задачи диаризации аудодорожки. Сэмплов достаточно много - поэтому будет переменная `total_amount`, которая будет брать уже от trimmed папки.\n",
    "2. Далее т.к. мы объединяем аудиодорожки то нам нужно их по отдельности обрезанное сохранить или запомнить их общую длину в csv файлы. В csv файле будет храниться столбец `min_length`который будет означать общую минимальную длину. Это поможет экономить место на диске. Но из за этого будет долше проходить инициализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41742431-8c6e-4eaf-80bd-9c850151f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_amount_percent = 0.1 # 10 %\n",
    "# Примерное число которое я хочу ~ 2000 записей для начало\n",
    "total_dataset_size = 10_000\n",
    "k = 2\n",
    "seed = k\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee4c5b-f9bb-4453-9226-135c2a5ae608",
   "metadata": {},
   "source": [
    "1. Число класстеров _k_\n",
    "2. _Seed_ зависит от числа кластеров. Это сделанно для того что бы добавить не предусмотренность. Но таким образом есть опасность что модель просто выучит голоса нежели начнет.\n",
    "3. Для каждого числа класстера можно брать разный датасет.\n",
    "4. Перемешивать нужно для того \"говорящие\" не шли одним за другим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103a32f8-1a0b-43f4-85fc-93c64756607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = glob(glob_paths['trimmed'])\n",
    "trimmed = [t.replace('\\\\', '/') for t in trimmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd473e47-93ea-40bf-a846-bb9571c4876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "folders_name = [('Train/tts', 0.25), ('Train/crowdsourced', 0.75)]\n",
    "total_folders = []\n",
    "for p_path, percent in folders_name:\n",
    "    temp_arr = find_files_folders(p_path, trimmed)\n",
    "    total_folders.append([random.sample(temp_arr, len(temp_arr)), percent])\n",
    "\n",
    "tracklist = []\n",
    "for p in total_folders: \n",
    "    if total_dataset_size * p[1] > len(p[0]):\n",
    "        raise Exception('Error, havent enough audios')\n",
    "        break\n",
    "    else:\n",
    "        tracklist.extend(p[0][:int(total_dataset_size * p[1])])\n",
    "\n",
    "random.shuffle(tracklist)\n",
    "print('Dataset size:', len(tracklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1124e0-e555-4f9a-a210-6ad4a906a79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:/ISSAI_KSC2_unpacked/trimmed/Train/crowdsourced/5f6a9888a3a71.flac',\n",
       " 'F:/ISSAI_KSC2_unpacked/trimmed/Train/crowdsourced/5f4765c4eb8f6.flac',\n",
       " 'F:/ISSAI_KSC2_unpacked/trimmed/Train/crowdsourced/5f16ca69c0ca0.flac',\n",
       " 'F:/ISSAI_KSC2_unpacked/trimmed/Train/crowdsourced/5ef96cf1a0fea.flac']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracklist[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b18d6dc-f9e2-4e72-8897-f48f89b1d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"{paths['root']}/train_tvnews_crowdsourced_tts_mixed_{k}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df426f-458e-4bdb-8306-fea06067aae8",
   "metadata": {},
   "source": [
    "Смешивание файлов\n",
    "> Этот метод не нравится т.к. хочется что бы дорожка была как можно длинее. То есть аудио с одинаковой длины объединялись с одинаковыми аудио."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf6caf-f488-4f47-a764-49bc448b5586",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### First attemp | Depricated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a070fec6-db10-4d37-946c-17f602344549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 5000/5000 [00:31<00:00, 161.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Этот метод не нравится т.к. хочется что бы дорожка была как можно длинее.\n",
    "# То есть аудио с одинаковой длины объединялись с одинаковыми аудио.\n",
    "\n",
    "# код работает только для двух аудиодорожек\n",
    "for i in tqdm(range(0, len(tracklist), k)):\n",
    "    tracks = tracklist[i:i+k]\n",
    "    tracks = sorted(tracks)\n",
    "    overlay, min_length = overlap_using_min(tracks)\n",
    "    audio_names = \"_\".join([get_file_name(t) for t in tracks])\n",
    "    audio_name = f\"{paths['root']}/train_tvnews_crowdsourced_tts_mixed_{k}/{audio_names}.flac\"\n",
    "    sf.write(f\"{audio_name}\", overlay, samplerate=16000)\n",
    "    df = pd.DataFrame({'mixed_audio': [audio_name],\n",
    "                       'common_len_idx': [min_length],\n",
    "                       **{f'audio_{j+1}': [t.replace('\\\\', '/')] for j, t in enumerate(tracks)}})\n",
    "    df.to_csv(f\"{audio_name.replace('.flac', '.csv')}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b3b18-305c-4fb1-bcc6-d27027d5025e",
   "metadata": {},
   "source": [
    "##### Second attemp | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37726156-a7a9-4e18-a32c-18bb13f4042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from align import relative_normalize_audio\n",
    "from scipy.signal import resample\n",
    "\n",
    "sample_rate = 16000\n",
    "common_folder = f\"{paths['root']}/diahard_data/train_tvnews_crowdsourced_tts_mixed_{k}\"\n",
    "os.makedirs(common_folder, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(0, len(tracklist), k)):\n",
    "    tracks = tracklist[i:i+k]\n",
    "    tracks = sorted(tracks)\n",
    "    # overlaping and normalize\n",
    "    signals = []\n",
    "    for audio_path in tracks:\n",
    "        signal, sr = librosa.load(audio_path, sr=None)\n",
    "        if sr != sample_rate:\n",
    "            signal = librosa.resample(signal, orig_sr=sr, target_sr=sample_rate)\n",
    "        signals.append(signal)\n",
    "        \n",
    "    min_length = min(len(audio) for audio in signals)\n",
    "    signals = [s[:min_length] for s in signals]\n",
    "    overlay, cleans = relative_normalize_audio(signals, snr)\n",
    "    \n",
    "    # Naming, saving '.flac' '.csv'\n",
    "    overlap_audio_name = f\"{common_path}mixed_{k}/{\"_\".join([get_file_name(t) for t in tracks])}.flac\" \n",
    "    print(overlap_audio_name)\n",
    "    # sf.write(f\"{overlap_audio_name}\", overlay, samplerate=sample_rate)\n",
    "    cleans_name = []\n",
    "    for j, t in enumerate(tracks):\n",
    "        os.makedirs(f\"{common_folder}/spk{j+1}\", exist_ok=True)\n",
    "        clean_name = f\"{common_folder}/spk{j + 1}/{get_file_name(tracks[j])}.flac\"\n",
    "        clean_names.append(clean_name)\n",
    "        sf.write(clean_name, cleans[j], samplerate=sample_rate)\n",
    "    df = pd.DataFrame({'mixed_audio': [overlap_audio_name],\n",
    "                       'common_len_idx': [min_length],\n",
    "                       **{f'audio_{j+1}': [clean_names[j].replace('\\\\', '/')] for j, t in enumerate(cleans_name)}})    \n",
    "    df.to_csv(f\"{overlap_audio_name.replace('.flac', '.csv')}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8f87a3-c45d-438c-b73f-a03c4b0aebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2330.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get full dataset DF\n",
    "annotations = glob(f\"F:/ISSAI_KSC2_unpacked/train_tvnews_crowdsourced_tts_mixed_{k}/*.csv\")\n",
    "all_dfs = []\n",
    "for csv in tqdm(annotations):\n",
    "    df = pd.read_csv(csv)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "concated_df = pd.concat(all_dfs).reset_index(drop=True)\n",
    "os.makedirs(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO\", exist_ok=True)\n",
    "concated_df.to_csv(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/CONCATED_DFS_train_crowdsourced_tvnews_tts=5000_k={k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da758c9-998e-4a71-b309-f0f133e397a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed num files: 5000 for k = 2 clusters, total_amount_files: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mixed num files: {len(concated_df)} for k = {k} clusters, total_amount_files: {len(tracklist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eea5f7-6151-419b-865e-baa9948a8c0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Count mean and min chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b9ca81-38c6-42ef-9fdd-e1a523fe4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = glob(glob_paths['trimmed'])\n",
    "trimmed = [t.replace('\\\\', '/') for t in trimmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7bd17f8-16a6-446a-9922-bc3265849b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 145231/145231 [01:10<00:00, 2048.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 145231/145231 [00:00<00:00, 8083643.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Минимальный размер (least_size): 7200 отсчетов\n",
      "Средний размер (chunk_size): 60511 отсчетов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Рассчитываем длительности аудиофайлов\n",
    "durations = []\n",
    "for t in tqdm(trimmed):\n",
    "    # Загружаем аудио с помощью librosa\n",
    "    audio, sample_rate = librosa.load(t, sr=None)\n",
    "    # Вычисляем длительность\n",
    "    durations.append(len(audio) / sample_rate)\n",
    "\n",
    "# Рассчитываем минимальный и средний размер chunk_size\n",
    "def calculate_chunk_sizes(durations, sample_rate):\n",
    "    \"\"\"\n",
    "    Рассчитывает минимальный и средний размер chunk size.\n",
    "\n",
    "    :param durations: Список длительностей аудио (в секундах).\n",
    "    :param sample_rate: Частота дискретизации (в Гц).\n",
    "    :return: Минимальный и средний размер chunk size (в отсчетах).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for duration in tqdm(durations):\n",
    "        samples.append(int(duration * sample_rate))\n",
    "    least_size = min(samples)\n",
    "    avg_chunk_size = sum(samples) // len(samples)\n",
    "    return least_size, avg_chunk_size\n",
    "\n",
    "# Вычисляем параметры\n",
    "sample_rate = 16000  # Если известно, можно задать свою частоту дискретизации\n",
    "least_size, avg_chunk_size = calculate_chunk_sizes(durations, sample_rate)\n",
    "\n",
    "print(f\"Минимальный размер (least_size): {least_size} отсчетов\")\n",
    "print(f\"Средний размер (chunk_size): {avg_chunk_size} отсчетов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a57d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### To get need to data-format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8af9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# папка где хранять исходники для дириазации - они обрезанные без начало 'F:/ISSAI_KSC2_unpacked/trimmed',\n",
    "# mixed_2 где хранять смешанные аудиодорожки для двух класстеров\n",
    "# осторожно - нету папки где храняться targetы - вот эти и займемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Создаем все targetы\n",
    "# 2. generate `.scp` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef0f6-7265-43e5-a5f2-e924800a4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Есть csv файл нужно \n",
    "# Я хочу сохранить audio_1, audio_2 в отдельные папки \n",
    "# Папки будет две - одна tr - train, другая cv - validation\n",
    "# В каждой внутри папки есть mix и "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b131587d-75ad-4b2d-aa53-78f249e6271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Делим один csv файла на два. \n",
    "k = 2\n",
    "csv_path = f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/CONCATED_DFS_tts=2000_k_{2}.csv\"\n",
    "all_df = pd.read_csv(csv_path)\n",
    "half_index = int(len(all_df) * 0.8)\n",
    "df1 = all_df.iloc[:half_index]\n",
    "df2 = all_df.iloc[half_index:]\n",
    "df1.to_csv(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/train_k_{k}.csv\", index=False)\n",
    "df2.to_csv(f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/valid_k_{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7094e2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e395c83dbae24d80a5f4342c832ee5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 2\n",
    "csv_path = f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/train_k_{k}.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    mixed_audio_path = row.iloc[0]\n",
    "    stage = get_file_name(csv_path).split('_')[0]\n",
    "    assert stage in ['train', 'valid'] \n",
    "    new_mx_audio_path = paths['dihard_data'] + f\"/mixed_{k}/{stage}/mix/\" + get_file_name (mixed_audio_path) + '.flac'\n",
    "    shutil.move(mixed_audio_path, new_mx_audio_path)\n",
    "    common_len = row.iloc[1]\n",
    "    for column in range(2, len(full_df.columns)):\n",
    "        raw_pth_name = row.iloc[column]\n",
    "        audio, sr = librosa.load(raw_pth_name, sr=sr)\n",
    "        stage = get_file_name(csv_path).split('_')[0]\n",
    "        assert stage in ['train', 'valid'] \n",
    "        folder_name = f\"{paths['dihard_data']}/mixed_{k}/{stage}/s{column-k+1}\"\n",
    "        new_path_file_name = f\"{folder_name}/{get_file_name(raw_pth_name)}.flac\"\n",
    "        sf.write(f\"{new_path_file_name}\", audio[:common_len], samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d74c7-12a4-4a71-bc8d-e9939722a81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "csv_path = f\"F:/ISSAI_KSC2_unpacked/DIHARD_DATA_INFO/valid_k_{k}.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    mixed_audio_path = row.iloc[0]\n",
    "    stage = get_file_name(csv_path).split('_')[0]\n",
    "    assert stage in ['train', 'valid'] \n",
    "    new_mx_audio_path = paths['dihard_data'] + f\"/mixed_{k}/{stage}/mix/\" + get_file_name (mixed_audio_path) + '.flac'\n",
    "    shutil.move(mixed_audio_path, new_mx_audio_path)\n",
    "    common_len = row.iloc[1]\n",
    "    for column in range(2, len(full_df.columns)):\n",
    "        raw_pth_name = row.iloc[column]\n",
    "        audio, sr = librosa.load(raw_pth_name, sr=sr)\n",
    "        stage = get_file_name(csv_path).split('_')[0]\n",
    "        assert stage in ['train', 'valid'] \n",
    "        folder_name = f\"{paths['dihard_data']}/mixed_{k}/{stage}/s{column-k+1}\"\n",
    "        new_path_file_name = f\"{folder_name}/{get_file_name(raw_pth_name)}.flac\"\n",
    "        sf.write(f\"{new_path_file_name}\", audio[:common_len], samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a53fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "import os\n",
    "\n",
    "# Пути и файлы для записи\n",
    "datasets = {\n",
    "    \"train\": {\n",
    "        \"mix\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/train/mix\",\n",
    "        \"s1\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/train/s1\",\n",
    "        \"s2\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/train/s2\",\n",
    "        \"scp\": {\n",
    "            \"mix\": \"tr_mix.scp\",\n",
    "            \"s1\": \"tr_s1.scp\",\n",
    "            \"s2\": \"tr_s2.scp\",\n",
    "        },\n",
    "    },\n",
    "    \"cv\": {\n",
    "        \"mix\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/valid/mix\",\n",
    "        \"s1\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/valid/s1\",\n",
    "        \"s2\": \"F:/ISSAI_KSC2_unpacked/diahard_data/mixed_2/valid/s2\",\n",
    "        \"scp\": {\n",
    "            \"mix\": \"cv_mix.scp\",\n",
    "            \"s1\": \"cv_s1.scp\",\n",
    "            \"s2\": \"cv_s2.scp\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Функция для записи данных в SCP-файл\n",
    "def write_scp(input_dir, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            files.sort()\n",
    "            for file in files:\n",
    "                f.write(f\"{file} {os.path.join(root, file)}\\n\")\n",
    "\n",
    "# Генерация SCP-файлов для всех наборов данных\n",
    "for split, paths in datasets.items():\n",
    "    for key, input_dir in paths.items():\n",
    "        if key == \"scp\":  # Пропустить словарь с именами файлов\n",
    "            continue\n",
    "        write_scp(input_dir, paths[\"scp\"][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4883c7-37b0-4671-bdcf-e4ef330f2be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
